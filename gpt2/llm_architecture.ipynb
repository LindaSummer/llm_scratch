{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192b1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    vocab_size: int = 50257\n",
    "    context_length: int = 1024\n",
    "    emb_dim: int = 768\n",
    "    num_heads: int = 12\n",
    "    num_layers: int = 12\n",
    "    dropout: float = 0.1\n",
    "    qkv_bias: bool = False\n",
    "    bias: bool = False\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, config: Config, \n",
    "                 transformer_cls=DummyTransformerBlock, \n",
    "                 norm_layer_cls=DummyLayerNorm):\n",
    "        super().__init__()\n",
    "        self._cfg = config\n",
    "        self._tok_emd = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "        self._pos_emd = nn.Embedding(config.context_length, config.emb_dim)\n",
    "        self._dropout = nn.Dropout(config.dropout)\n",
    "        self._transformer_blocks = nn.Sequential(\n",
    "            *[transformer_cls(config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "        self._final_norm = norm_layer_cls(config)\n",
    "        self._out_head = nn.Linear(config.emb_dim, config.vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, in_idx: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        device = in_idx.device\n",
    "        tok_embeds = self._tok_emd(in_idx)\n",
    "        pos_embeds = self._pos_emd(\n",
    "            torch.arange(seq_len, device=device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self._dropout(x)\n",
    "        x = self._transformer_blocks(x)\n",
    "        x = self._final_norm(x)\n",
    "        return self._out_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db887f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([19693,   345,   460,  5967,   318,  1103,    13]), tensor([ 8890,   489,  8467,   318,   262,  8713, 44809,    13])]\n",
      "tensor([[19693,   345,   460,  5967,   318,  1103,    13, 50256],\n",
      "        [ 8890,   489,  8467,   318,   262,  8713, 44809,    13]])\n",
      "torch.Size([2, 8, 50257])\n",
      "Simplicity is the ultimate sophistication.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "config = Config()\n",
    "txts = [\"Everything you can imagine is real.\", \"Simplicity is the ultimate sophistication.\"]\n",
    "batch = [torch.tensor(tokenizer.encode(txt)) for txt in txts]\n",
    "print(batch)\n",
    "batch = nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=tokenizer.eot_token)\n",
    "print(batch)\n",
    "model = DummyGPTModel(config)\n",
    "logits = model(batch)\n",
    "print(logits.shape)  # Expected output: torch.Size([2, 8, 50257])\n",
    "decoded = tokenizer.decode(batch[1].tolist())\n",
    "print(decoded)  # Decoded text from the first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbafcdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "torch.manual_seed(123)\n",
    "GPT_CONFIG_124M = Config()\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd3b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config: Config, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self._eps = eps\n",
    "        emb_dim = config.emb_dim\n",
    "        self._scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self._shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self._eps)\n",
    "        return self._scale * norm_x + self._shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1533e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.7862,  0.2201, -0.4505,  ..., -0.9930, -0.1411, -0.2997],\n",
      "         [-0.0788,  0.3002, -0.2933,  ...,  0.1582,  0.8911,  0.8224],\n",
      "         [ 0.3706,  1.1119, -0.3223,  ...,  0.8017, -0.0038,  0.3932],\n",
      "         [ 0.0636,  1.0565, -0.2506,  ...,  0.7537, -0.0750, -0.6892]],\n",
      "\n",
      "        [[-0.7203,  0.1351, -0.6010,  ..., -1.0265,  0.1728, -0.2918],\n",
      "         [-0.5934,  0.4450, -0.0059,  ...,  0.3412,  0.0572,  1.0979],\n",
      "         [ 0.2673,  0.8401, -0.4473,  ..., -0.0181, -0.1089,  0.2539],\n",
      "         [-0.1034, -0.5897, -0.3929,  ...,  1.4013, -0.3186,  0.1303]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch = [\n",
    "    torch.tensor(tokenizer.encode(txt1)),\n",
    "    torch.tensor(tokenizer.encode(txt2)),\n",
    "]\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "torch.manual_seed(123)\n",
    "GPT_CONFIG_124M = Config()\n",
    "model = DummyGPTModel(GPT_CONFIG_124M, norm_layer_cls=LayerNorm)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7e18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        d_in = d_out = config.emb_dim\n",
    "        assert (d_out % config.num_heads == 0), \"d_out must be divisiable by num_heads\"\n",
    "        super().__init__()\n",
    "        self._d_in = d_in\n",
    "        self._d_out = d_out\n",
    "        self._num_heads = config.num_heads\n",
    "        self._d_head = d_out // config.num_heads\n",
    "        self._w_q = nn.Linear(self._d_in, self._d_out, bias=config.qkv_bias)\n",
    "        self._w_k = nn.Linear(self._d_in, self._d_out, bias=config.qkv_bias)\n",
    "        self._w_v = nn.Linear(self._d_in, self._d_out, bias=config.qkv_bias)\n",
    "        self._out_proj = nn.Linear(self._d_out, self._d_out)\n",
    "        self._dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(config.context_length, config.context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        import math\n",
    "        \n",
    "        q = self._w_q(x)\n",
    "        k = self._w_k(x)\n",
    "        v = self._w_v(x)\n",
    "        \n",
    "        b, context_length, d_out = q.shape\n",
    "        split_view = lambda x: x.view(b, context_length, self._num_heads, self._d_head)\\\n",
    "            .transpose(1, 2)\n",
    "        q = split_view(q)\n",
    "        k = split_view(k)\n",
    "        v = split_view(v)\n",
    "        \n",
    "        attn_scores = q @ k.transpose(-1, -2) / math.sqrt(k.shape[-1])\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:context_length, :context_length], -torch.inf)\n",
    "        weight = torch.softmax(attn_scores, dim=-1)\n",
    "        weight = self._dropout(weight)\n",
    "        \n",
    "        z = weight @ v\n",
    "        z = z.transpose(1, 2).contiguous().view(b, context_length, d_out)\n",
    "        z = self._out_proj(z)\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c63a276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 config: Config,\n",
    "                 ff_mid_dim:int=0,\n",
    "                 ff_activation=nn.GELU()):\n",
    "        super().__init__()\n",
    "        if ff_mid_dim == 0:\n",
    "            ff_mid_dim = 4 * config.emb_dim\n",
    "        self._layers = nn.Sequential(\n",
    "            nn.Linear(config.emb_dim, ff_mid_dim),\n",
    "            ff_activation,\n",
    "            nn.Linear(ff_mid_dim, config.emb_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self._layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: Config, \n",
    "                 norm_layer_cls=LayerNorm,\n",
    "                 feed_fwd_cls=FeedForward,\n",
    "                 dropouts=None):\n",
    "        if dropouts is None:\n",
    "            dropouts = [config.dropout, config.dropout]\n",
    "        super().__init__()\n",
    "        self._norm_1 = norm_layer_cls(config=config)\n",
    "        self._attention = MultiHeadAttention(config=config)\n",
    "        self._drop_1 = nn.Dropout(dropouts[0])\n",
    "        self._norm_2 = norm_layer_cls(config=config)\n",
    "        self._ff = feed_fwd_cls(config=config)\n",
    "        self._drop_2 = nn.Dropout(dropouts[1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self._norm_1(x)\n",
    "        x = self._attention(x)\n",
    "        x = self._drop_1(x)\n",
    "        x += shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self._norm_2(x)\n",
    "        x = self._ff(x)\n",
    "        x = self._drop_2(x)\n",
    "        x += shortcut\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e87d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 config: Config,\n",
    "                 gpt_dropout=-1,\n",
    "                 transformer_cls=TransformerBlock,\n",
    "                 norm_layer_cls=LayerNorm):\n",
    "        super().__init__()\n",
    "        self._tok_emd = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "        self._pos_emd = nn.Embedding(config.context_length, config.emb_dim)\n",
    "        if gpt_dropout < 0:\n",
    "            gpt_dropout = config.dropout\n",
    "        self._dropout = nn.Dropout(gpt_dropout)\n",
    "        self._transformers = nn.Sequential(\n",
    "            *[transformer_cls(config=config) for _ in range(config.num_layers)]\n",
    "        )\n",
    "        self._final_norm_layer = norm_layer_cls(config=config)\n",
    "        self._out_head = nn.Linear(config.emb_dim, config.vocab_size, bias=config.bias)\n",
    "        \n",
    "    def forward(self, in_idx: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        token_embeds = self._tok_emd(in_idx)\n",
    "        pos_embeds = self._pos_emd(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = token_embeds + pos_embeds\n",
    "        x = self._dropout(x)\n",
    "        x = self._transformers(x)\n",
    "        x = self._final_norm_layer(x)\n",
    "        return self._out_head(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17657936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3612,  0.4223, -0.0709,  ...,  0.3479,  0.4655, -0.2833],\n",
      "         [-0.1786, -0.5656, -0.9478,  ...,  0.0475,  0.5173, -0.3161],\n",
      "         [ 0.7118,  0.0335,  0.1078,  ...,  0.1019, -0.4330, -0.2547],\n",
      "         [-1.0068,  0.3421, -0.1191,  ...,  0.7194,  0.4018,  0.0532]],\n",
      "\n",
      "        [[-0.2562,  0.0899,  0.0337,  ...,  0.2659,  0.4448, -0.6800],\n",
      "         [ 0.1229,  0.3651, -0.2071,  ...,  0.7703,  0.2702,  0.2249],\n",
      "         [ 1.0556,  1.0312, -0.2797,  ...,  0.6933,  0.3201, -0.3172],\n",
      "         [-0.1560,  0.3924,  0.3286,  ...,  1.2626, -0.1862,  0.0392]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# demo_config = Config(context_length=4)\n",
    "model = GPTModel(config=GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053aa1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens=20, context_size=10):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "model=model,\n",
    "idx=encoded_tensor,\n",
    "max_new_tokens=6,\n",
    "context_size=GPT_CONFIG_124M.context_length,\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
